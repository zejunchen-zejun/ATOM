# SPDX-License-Identifier: MIT
# Copyright (C) 2024-2025, Advanced Micro Devices, Inc. All rights reserved.

import argparse
import asyncio
import json
import logging
import queue
import time
import uuid
from collections import defaultdict
from contextlib import asynccontextmanager
from typing import Any, AsyncGenerator, Dict, List, Optional

import uvicorn
from atom import SamplingParams
from atom.model_engine.arg_utils import EngineArgs
from atom.model_engine.request import RequestOutput
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from transformers import AutoTokenizer

logger = logging.getLogger("atom")


class ChatMessage(BaseModel):
    role: str
    content: str
    
    class Config:
        extra = "allow"


class ChatCompletionRequest(BaseModel):
    model: Optional[str] = None
    messages: Optional[List[ChatMessage]] = None
    prompt: Optional[List[ChatMessage]] = None  # Accept 'prompt' as alias for 'messages'
    temperature: Optional[float] = 1.0
    top_p: Optional[float] = 1.0
    max_tokens: Optional[int] = 256
    stop: Optional[List[str]] = None
    ignore_eos: Optional[bool] = False
    stream: Optional[bool] = False
    seed: Optional[int] = None
    
    def get_messages(self) -> List[ChatMessage]:
        """Get messages from either 'messages' or 'prompt' field"""
        if self.messages is not None:
            return self.messages
        elif self.prompt is not None:
            return self.prompt
        else:
            raise ValueError("Either 'messages' or 'prompt' field is required")


class CompletionRequest(BaseModel):
    model: Optional[str] = None
    prompt: str
    temperature: Optional[float] = 1.0
    top_p: Optional[float] = 1.0
    max_tokens: Optional[int] = 256
    stop: Optional[List[str]] = None
    ignore_eos: Optional[bool] = False
    stream: Optional[bool] = False


class ChatCompletionResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[Dict[str, Any]]
    usage: Dict[str, Any]
    
    class Config:
        extra = "allow"


class CompletionResponse(BaseModel):
    id: str
    object: str = "text_completion"
    created: int
    model: str
    choices: List[Dict[str, Any]]
    usage: Dict[str, Any]


engine = None
tokenizer: Optional[AutoTokenizer] = None
model_name: str = ""
_stream_queues: Dict[str, queue.Queue] = {}
_seq_id_to_request_id: Dict[int, str] = {}


def create_chat_completion_chunk(
    request_id: str,
    model: str,
    content: str = "",
    finish_reason: Optional[str] = None,
    usage: Optional[Dict] = None,
) -> str:
    chunk = {
        "id": request_id,
        "object": "chat.completion.chunk",
        "created": int(time.time()),
        "model": model,
        "choices": [
            {
                "index": 0,
                "delta": {"content": content} if content else {},
                "finish_reason": finish_reason,
                "logprobs": None,
            }
        ],
    }
    if usage is not None:
        chunk["usage"] = usage
    return f"data: {json.dumps(chunk)}\n\n"


def create_chat_usage_chunk(request_id: str, model: str, usage: Dict) -> str:
    """Create a chunk containing only usage information for chat completions (no choices)."""
    chunk = {
        "id": request_id,
        "object": "chat.completion.chunk",
        "created": int(time.time()),
        "model": model,
        "usage": usage,
    }
    return f"data: {json.dumps(chunk)}\n\n"


def create_completion_chunk(
    request_id: str,
    model: str,
    text: str,
    finish_reason: Optional[str] = None,
    usage: Optional[Dict] = None,
) -> str:
    chunk = {
        "id": request_id,
        "object": "text_completion",
        "created": int(time.time()),
        "model": model,
        "choices": [
            {"index": 0, "text": text, "finish_reason": finish_reason, "logprobs": None}
        ],
    }
    if usage is not None:
        chunk["usage"] = usage
    return f"data: {json.dumps(chunk)}\n\n"


def create_usage_chunk(request_id: str, model: str, usage: Dict) -> str:
    """Create a chunk containing only usage information (no choices)."""
    chunk = {
        "id": request_id,
        "object": "text_completion",
        "created": int(time.time()),
        "model": model,
        "usage": usage,
    }
    return f"data: {json.dumps(chunk)}\n\n"


def _send_stream_chunk_direct(request_output: RequestOutput, request_id: str, stream_queue: queue.Queue):
    """Send stream chunk directly without global mapping lookup - avoids race condition"""
    global tokenizer
    # Decode the new tokens
    new_text = tokenizer.decode(request_output.output_tokens, skip_special_tokens=True)
    logger.debug(
        f"send_stream_chunk_direct: seq_id={request_output.request_id}, request_id={request_id}, tokens={request_output.output_tokens}, text='{new_text[:50]}...'"
    )

    # Prepare chunk data
    chunk_data = {
        "text": new_text,
        "token_ids": request_output.output_tokens,
        "finished": request_output.finished,
        "finish_reason": request_output.finish_reason,
    }

    try:
        stream_queue.put_nowait(chunk_data)
    except queue.Full:
        logger.warning(
            f"send_stream_chunk_direct: Queue full for request_id {request_id}, skipping chunk"
        )


def send_stream_chunk(request_output: RequestOutput):
    global tokenizer, _stream_queues, _seq_id_to_request_id

    request_id = _seq_id_to_request_id.get(request_output.request_id)
    if request_id is None:
        logger.warning(
            f"send_stream_chunk: No request_id found for sequence {request_output.request_id}"
        )
        return

    stream_queue = _stream_queues.get(request_id)
    if stream_queue is None:
        logger.warning(f"send_stream_chunk: No queue found for request_id {request_id}")
        return

    _send_stream_chunk_direct(request_output, request_id, stream_queue)


async def generate_async(
    prompt: str, sampling_params: SamplingParams, request_id: str
) -> AsyncGenerator[Dict[str, Any], None]:
    global engine, tokenizer
    async for output in engine.generate_async(prompt, sampling_params, request_id):
        yield {
            "text": output["text"],
            "token_ids": output["token_ids"],
            "finish_reason": output.get("finish_reason"),
            "num_tokens_input": len(tokenizer.encode(prompt)),
            "num_tokens_output": len(output["token_ids"]),
            "ttft": output.get("ttft", 0.0),
            "tpot": output.get("tpot", 0.0),
            "latency": output.get("latency", 0.0),
        }


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Lifespan context manager for startup and shutdown events."""
    # Startup
    logger.info("Server started successfully and ready to accept requests")
    print("Server started successfully and ready to accept requests!")
    yield
    # Shutdown (if needed in the future)


app = FastAPI(title="Atom OpenAI API Server", lifespan=lifespan)


@app.post("/v1/chat/completions")
async def chat_completions(request: ChatCompletionRequest):
    global engine, tokenizer, model_name
    if request.model is not None and request.model != model_name:
        raise HTTPException(
            status_code=400,
            detail=f"Requested model '{request.model}' does not match server model '{model_name}'",
        )
    try:
        messages = request.get_messages()
        prompt = tokenizer.apply_chat_template(
            [{"role": msg.role, "content": msg.content} for msg in messages],
            tokenize=False,
            add_generation_prompt=True,
        )

        sampling_params = SamplingParams(
            temperature=request.temperature,
            max_tokens=request.max_tokens,
            stop_strings=request.stop,
            ignore_eos=request.ignore_eos,
        )

        request_id = f"chatcmpl-{uuid.uuid4().hex}"
        created = int(time.time())
        if request.stream:
            stream_queue = queue.Queue()
            _stream_queues[request_id] = stream_queue

            captured_request_id = request_id
            captured_stream_queue = stream_queue
            
            def stream_callback(request_output: RequestOutput):
                _send_stream_chunk_direct(request_output, captured_request_id, captured_stream_queue)

            loop = asyncio.get_event_loop()

            def do_preprocess():
                seq = engine.io_processor.preprocess(
                    prompt, sampling_params, stream_callback=stream_callback
                )
                _seq_id_to_request_id[seq.id] = captured_request_id
                return seq

            seq = await loop.run_in_executor(None, do_preprocess)

            seq_id = seq.id

            logger.info(
                f"API: Created request_id={request_id}, seq_id={seq_id}, queue={stream_queue is not None}"
            )
            engine.core_mgr.add_request([seq])
            logger.info(
                f"API: Added request to engine, callback registered: {seq.stream_callback is not None}"
            )

            async def generate_stream():

                prev_text = ""
                num_tokens_input = len(tokenizer.encode(prompt))
                num_tokens_output = 0
                yield create_chat_completion_chunk(request_id, model_name, "")

                # Consume chunks from queue using executor to avoid blocking
                finished = False
                loop = asyncio.get_event_loop()
                while not finished:
                    chunk_data = await loop.run_in_executor(None, stream_queue.get)
                    new_text = chunk_data["text"]
                    current_text = prev_text + new_text
                    new_content = new_text
                    prev_text = current_text

                    chunk_token_ids = chunk_data.get("token_ids", [])
                    num_tokens_output += len(chunk_token_ids)

                    yield create_chat_completion_chunk(
                        request_id,
                        model_name,
                        new_content,
                        finish_reason=chunk_data.get("finish_reason"),
                    )

                    if chunk_data.get("finished", False):
                        finished = True

                _stream_queues.pop(request_id, None)
                _seq_id_to_request_id.pop(seq_id, None)
                # Clean up pending request from io_processor
                engine.io_processor.requests.pop(seq_id, None)

                yield create_chat_completion_chunk(request_id, model_name, "", "stop")
                usage = {
                    "prompt_tokens": num_tokens_input,
                    "completion_tokens": num_tokens_output,
                    "total_tokens": num_tokens_input + num_tokens_output,
                }
                yield create_chat_usage_chunk(request_id, model_name, usage)
                yield "data: [DONE]\n\n"

            return StreamingResponse(generate_stream(), media_type="text/event-stream")

        # non-streaming
        final_output = None
        async for output in generate_async(prompt, sampling_params, request_id):
            final_output = output
        if final_output is None:
            raise RuntimeError("No output generated")

        response_data = ChatCompletionResponse(
            id=request_id,
            created=created,
            model=model_name,
            choices=[
                {
                    "index": 0,
                    "message": {"role": "assistant", "content": final_output["text"]},
                    "finish_reason": final_output["finish_reason"],
                    "text": final_output["text"],
                }
            ],
            usage={
                "prompt_tokens": final_output["num_tokens_input"],
                "completion_tokens": final_output["num_tokens_output"],
                "total_tokens": final_output["num_tokens_input"]
                + final_output["num_tokens_output"],
                "ttft_s": round(final_output.get("ttft", 0.0), 4),
                "tpot_s": round(final_output.get("tpot", 0.0), 4),
                "latency_s": round(final_output.get("latency", 0.0), 4),
            },
        )
        return response_data
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/v1/completions")
async def completions(request: CompletionRequest):
    global engine, tokenizer, model_name
    if request.model is not None and request.model != model_name:
        raise HTTPException(
            status_code=400,
            detail=f"Requested model '{request.model}' does not match server model '{model_name}'",
        )
    try:
        sampling_params = SamplingParams(
            temperature=request.temperature,
            max_tokens=request.max_tokens,
            stop_strings=request.stop,
            ignore_eos=request.ignore_eos,
        )

        if request.stream:
            request_id = f"cmpl-{uuid.uuid4().hex}"

            stream_queue = queue.Queue()
            _stream_queues[request_id] = stream_queue

            captured_request_id = request_id
            captured_stream_queue = stream_queue
            
            def stream_callback(request_output: RequestOutput):
                _send_stream_chunk_direct(request_output, captured_request_id, captured_stream_queue)

            loop = asyncio.get_event_loop()

            def do_preprocess():
                seq = engine.io_processor.preprocess(
                    request.prompt, sampling_params, stream_callback=stream_callback
                )
                _seq_id_to_request_id[seq.id] = captured_request_id
                return seq

            seq = await loop.run_in_executor(None, do_preprocess)

            seq_id = seq.id
            logger.info(
                f"API: Created request_id={request_id}, seq_id={seq_id}, queue={stream_queue is not None}"
            )
            engine.core_mgr.add_request([seq])
            logger.info(
                f"API: Added request to engine, callback registered: {seq.stream_callback is not None}"
            )

            async def generate_stream():
                prev_text = ""
                num_tokens_input = len(tokenizer.encode(request.prompt))
                num_tokens_output = 0

                finished = False
                loop = asyncio.get_event_loop()
                while not finished:
                    chunk_data = await loop.run_in_executor(None, stream_queue.get)
                    new_text = chunk_data["text"]
                    current_text = prev_text + new_text
                    new_content = new_text
                    prev_text = current_text

                    chunk_token_ids = chunk_data.get("token_ids", [])
                    num_tokens_output += len(chunk_token_ids)

                    yield create_completion_chunk(
                        request_id,
                        model_name,
                        new_content,
                        finish_reason=chunk_data.get("finish_reason"),
                    )

                    if chunk_data.get("finished", False):
                        finished = True

                # Cleanup
                _stream_queues.pop(request_id, None)
                if seq_id in _seq_id_to_request_id:
                    _seq_id_to_request_id.pop(seq_id, None)
                # Clean up pending request from io_processor
                engine.io_processor.requests.pop(seq_id, None)

                yield create_completion_chunk(request_id, model_name, "", "stop")
                usage = {
                    "prompt_tokens": num_tokens_input,
                    "completion_tokens": num_tokens_output,
                    "total_tokens": num_tokens_input + num_tokens_output,
                }
                yield create_usage_chunk(request_id, model_name, usage)
                yield "data: [DONE]\n\n"

            return StreamingResponse(generate_stream(), media_type="text/event-stream")

        # non-streaming
        request_id = f"cmpl-{uuid.uuid4().hex}"
        final_output = None
        async for output in generate_async(request.prompt, sampling_params, request_id):
            final_output = output
        if final_output is None:
            raise RuntimeError("No output generated")

        return CompletionResponse(
            id=request_id,
            created=int(time.time()),
            model=model_name,
            choices=[
                {
                    "index": 0,
                    "text": final_output["text"],
                    "finish_reason": final_output["finish_reason"],
                }
            ],
            usage={
                "prompt_tokens": final_output["num_tokens_input"],
                "completion_tokens": final_output["num_tokens_output"],
                "total_tokens": final_output["num_tokens_input"]
                + final_output["num_tokens_output"],
                "ttft_s": round(final_output.get("ttft", 0.0), 4),
                "tpot_s": round(final_output.get("tpot", 0.0), 4),
                "latency_s": round(final_output.get("latency", 0.0), 4),
            },
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/v1/models")
async def list_models():
    global model_name
    return {
        "object": "list",
        "data": [
            {
                "id": model_name,
                "object": "model",
                "created": int(time.time()),
                "owned_by": "atom",
            }
        ],
    }


@app.get("/health")
async def health():
    return {"status": "ok"}


@app.post("/start_profile")
async def start_profile():
    global engine
    try:
        engine.start_profile()
        return {"status": "success", "message": "Profiling started"}
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Failed to start profiling: {str(e)}"
        )


@app.post("/stop_profile")
async def stop_profile():
    global engine
    try:
        engine.stop_profile()
        return {
            "status": "success",
            "message": "Profiling stopped. Trace files generated.",
        }
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Failed to stop profiling: {str(e)}"
        )


def main():
    global engine, tokenizer, model_name
    parser = argparse.ArgumentParser(description="Atom OpenAI API Server")
    EngineArgs.add_cli_args(parser)
    parser.add_argument("--host", type=str, default="0.0.0.0", help="Server host")
    parser.add_argument(
        "--server-port",
        type=int,
        default=8000,
        help="Server port (note: --port is used for internal engine communication)",
    )
    args = parser.parse_args()

    print(f"Loading tokenizer from {args.model}...")
    tokenizer = AutoTokenizer.from_pretrained(args.model)
    model_name = args.model

    print(f"Initializing async engine with model {args.model}...")
    engine_args = EngineArgs.from_cli_args(args)
    engine = engine_args.create_async_engine()

    print(f"Starting server on {args.host}:{args.server_port}...")
    uvicorn.run(app, host=args.host, port=args.server_port)


if __name__ == "__main__":
    main()
